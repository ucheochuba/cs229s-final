{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yub2dizz6-7o"
      },
      "outputs": [],
      "source": [
        "# Choose variant and machine type\n",
        "VARIANT = '2b-it' #@param ['2b', '2b-it', '9b', '9b-it', '27b', '27b-it']\n",
        "MACHINE_TYPE = 'cuda' #@param ['cuda', 'cpu']\n",
        "PRINT_INFO = False #@param bool\n",
        "\n",
        "CONFIG = VARIANT[:2]\n",
        "if CONFIG == '2b':\n",
        "  CONFIG = '2b-v2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rWN_4WGf3lc"
      },
      "outputs": [],
      "source": [
        "# Choose seed and PAWS parameters\n",
        "SEED = 42 # @param int the answer to life, the universe, and everything\n",
        "\n",
        "USE_PAWS = True #@param bool\n",
        "PAWS_PERCENTILE = 25 # @param int\n",
        "\n",
        "paws_dict_path = \"paws_dict/gsm8k_paws_dict.json\"# @param str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gc4h8TnN7JfU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata # `userdata` is a Colab API.\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = 'ucheochuba'\n",
        "os.environ[\"KAGGLE_KEY\"] = '5f5dd974f13fe89c391f02b0cf74e892'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXSK8xC--QVk"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U torch immutabledict sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtKlR0cdaRE2",
        "outputId": "39e7510b-776b-4352-d08a-2d2e468b9d60",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5udufivS6-7p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import kagglehub\n",
        "from datasets import load_dataset\n",
        "from evaluate import load  # Use evaluate instead of load_metric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GbWpIcnF6-7p",
        "outputId": "eb12adf1-bcc0-4f2f-c234-c98fb307edba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded weights to: /kaggle/input/gemma-2/pytorch/gemma-2-2b-it/1\n"
          ]
        }
      ],
      "source": [
        "# Download weights directory from Kaggle\n",
        "try:\n",
        "    weights_dir = kagglehub.model_download(f'google/gemma-2/pyTorch/gemma-2-{VARIANT}')\n",
        "    print(f\"Downloaded weights to: {weights_dir}\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to download model weights: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIa09OlJgAUh",
        "outputId": "3553cccb-2a21-41db-f763-31e43b227a9c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reinitialized existing Git repository in /content/.git/\n",
            "error: remote origin already exists.\n",
            "From https://github.com/ucheochuba/cs229s-final\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        " # clone in PAWS threshold dictionary\n",
        "# Initialize a new git repository\n",
        "!git init\n",
        "\n",
        "# Add the remote repository\n",
        "!git remote add origin https://github.com/ucheochuba/cs229s-final.git\n",
        "\n",
        "# Enable sparse-checkout\n",
        "!git config core.sparseCheckout true\n",
        "\n",
        "# Set the folder path you want to checkout (e.g., paws_dict)\n",
        "!echo \"paws_dict/*\" >> .git/info/sparse-checkout\n",
        "\n",
        "# Pull the specific folder from the repository\n",
        "!git pull origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zxvX8aL6-7q",
        "outputId": "21c1eecf-d34b-4d5a-b35f-3a3595de789b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'gemma_pytorch' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# NOTE: The \"installation\" is just cloning the repo.\n",
        "# !git clone https://github.com/google/gemma_pytorch.git\n",
        "!git clone https://github.com/ucheochuba/paws_gemma_pytorch gemma_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU-2UZG09I9M",
        "outputId": "2fece31c-7b94-480c-cbbc-9b0cc18fb52e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/usr/local/lib/python3.10/dist-packages/setuptools/_vendor', '/root/.ipython', '/tmp/tmp1715yo2i', 'gemma_pytorch', 'gemma_pytorch']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('gemma_pytorch')\n",
        "print(sys.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38PWqOvy7Hvb"
      },
      "outputs": [],
      "source": [
        "from gemma.model import GemmaForCausalLM\n",
        "from gemma.tokenizer import Tokenizer\n",
        "from gemma.config import get_model_config\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbjwizlG6-7q",
        "outputId": "16e7ab0a-4ea8-4c46-c471-da526913982f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer and checkpoint files verified.\n"
          ]
        }
      ],
      "source": [
        "# Verify the presence of tokenizer and checkpoint files\n",
        "tokenizer_path = os.path.join(weights_dir, 'tokenizer.model')\n",
        "if not os.path.isfile(tokenizer_path):\n",
        "    raise FileNotFoundError(f\"Tokenizer not found at: {tokenizer_path}\")\n",
        "\n",
        "ckpt_path = os.path.join(weights_dir, 'model.ckpt')\n",
        "if not os.path.isfile(ckpt_path):\n",
        "    raise FileNotFoundError(f\"PyTorch checkpoint not found at: {ckpt_path}\")\n",
        "\n",
        "print(\"Tokenizer and checkpoint files verified.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwKO2lkmQGqr"
      },
      "source": [
        "### PAWS Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0yP1MEZQGHS"
      },
      "outputs": [],
      "source": [
        "# load in files\n",
        "import json\n",
        "\n",
        "with open(paws_dict_path, 'r') as f:\n",
        "    paws_dict = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJHqZutOYQal"
      },
      "outputs": [],
      "source": [
        "# Function to convert keys to integers recursively\n",
        "def convert_keys_to_int(d):\n",
        "    if isinstance(d, dict):\n",
        "        # Create a new dictionary with integer keys\n",
        "        return {int(k) if k.isdigit() else k: convert_keys_to_int(v) for k, v in d.items()}\n",
        "    elif isinstance(d, list):\n",
        "        # If the value is a list, recursively process each element\n",
        "        return [convert_keys_to_int(item) for item in d]\n",
        "    else:\n",
        "        return d\n",
        "\n",
        "# convert keys from str to int\n",
        "paws_dict = convert_keys_to_int(paws_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znsGjWr6-7r",
        "outputId": "457ede6a-4b53-441a-d095-fa6fa076cb2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing attention layer 0 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 1 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 2 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 3 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 4 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 5 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 6 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 7 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 8 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 9 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 10 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 11 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 12 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 13 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 14 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 15 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 16 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 17 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 18 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 19 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 20 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 21 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 22 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 23 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 24 with PAWS activated. Cutoff percentile: 25.\n",
            "Initializing attention layer 25 with PAWS activated. Cutoff percentile: 25.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GemmaForCausalLM(\n",
              "  (embedder): Embedding()\n",
              "  (model): GemmaModel(\n",
              "    (layers): ModuleList(\n",
              "      (0-25): 26 x Gemma2DecoderLayer(\n",
              "        (self_attn): GemmaAttention(\n",
              "          (qkv_proj): Linear()\n",
              "          (o_proj): Linear()\n",
              "        )\n",
              "        (mlp): GemmaMLP(\n",
              "          (gate_proj): Linear()\n",
              "          (up_proj): Linear()\n",
              "          (down_proj): Linear()\n",
              "        )\n",
              "        (input_layernorm): RMSNorm()\n",
              "        (post_attention_layernorm): RMSNorm()\n",
              "        (pre_feedforward_layernorm): RMSNorm()\n",
              "        (post_feedforward_layernorm): RMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): RMSNorm()\n",
              "  )\n",
              "  (sampler): Sampler()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "dataset = load_dataset(\"gsm8k\", \"main\", cache_dir='/tmp')\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "# Load Gemma 2B model\n",
        "MODEL_VARIANT = \"2b-v2\"  # Update if needed\n",
        "model_config = get_model_config(MODEL_VARIANT)\n",
        "model_config.tokenizer = tokenizer_path\n",
        "model_config.quant = 'quant' in VARIANT\n",
        "\n",
        "# Load tokenizer and model\n",
        "# tokenizer = Tokenizer(tokenizer_path)\n",
        "torch.set_default_dtype(model_config.get_dtype())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GemmaForCausalLM(model_config, use_paws=USE_PAWS, paws_percentile=PAWS_PERCENTILE, paws_dict=paws_dict)\n",
        "model.load_weights(ckpt_path)\n",
        "model.to(device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BetDCHgVcKp_"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "# from compute_score import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cXoCKMi9EXir"
      },
      "outputs": [],
      "source": [
        "# @title GSM8K Prompts\n",
        "\n",
        "PREAMBLE = \"\"\"As an expert problem solver solve step by step the following mathematical questions.\"\"\"\n",
        "\n",
        "# The default gsm8k prompt from the CoT paper\n",
        "# https://arxiv.org/pdf/2201.11903.pdf page 35.\n",
        "\n",
        "PROMPT = \"\"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
        "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted. So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
        "\n",
        "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
        "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
        "\n",
        "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
        "A: Leah had 32 chocolates and Leah's sister had 42. That means there were originally 32 + 42 = 74 chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
        "\n",
        "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
        "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
        "\n",
        "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
        "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so in total he has 7 + 2 = 9 toys. The answer is 9.\n",
        "\n",
        "Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
        "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29.\n",
        "\n",
        "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
        "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
        "\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: She bought 5 bagels for $3 each. This means she spent 5 * $3 = $15 on the bagels. She had $23 in beginning, so now she has $23 - $15 = $8. The answer is 8.\"\"\"\n",
        "\n",
        "\n",
        "# Extension of the default 8-shot prompt, page 35 in\n",
        "# https://arxiv.org/pdf/2201.11903.pdf\n",
        "# The extension is intended to improve performance on\n",
        "# more complicated gsm8k examples.\n",
        "\n",
        "EXTRA_3_SHOTS = \"\"\"As an expert problem solver solve step by step the following mathematical questions.\n",
        "\n",
        "Q: Tina makes $18.00 an hour.  If she works more than 8 hours per shift, she is eligible for overtime, which is paid by your hourly wage + 1/2 your hourly wage.  If she works 10 hours every day for 5 days, how much money does she make?\n",
        "A: Here's how to calculate Tina's earnings:\n",
        "\n",
        "**Regular Time:**\n",
        "- Hours per shift: 8 hours\n",
        "- Wage per hour: $18.00\n",
        "- Regular pay per shift: 8 hours * $18.00/hour = $144.00\n",
        "\n",
        "**Overtime:**\n",
        "- Overtime hours per shift: 10 hours - 8 hours = 2 hours\n",
        "- Overtime pay per hour: $18.00 + ($18.00 / 2) = $27.00\n",
        "- Overtime pay per shift: 2 hours * $27.00/hour = $54.00\n",
        "\n",
        "**Total per day:**\n",
        "- Regular pay + overtime pay: $144.00/shift + $54.00/shift = $198.00/day\n",
        "\n",
        "**Total for 5 days:**\n",
        "- 5 days * $198.00/day = $990.00\n",
        "\n",
        "**Therefore, Tina will make $990.00 in 5 days.** The answer is 990.\n",
        "\n",
        "Q: Abigail is trying a new recipe for a cold drink. It uses 1/4 of a cup of iced tea and 1 and 1/4 of a cup of lemonade to make one drink. If she fills a pitcher with 18 total cups of this drink, how many cups of lemonade are in the pitcher?\n",
        "A: ## Ambiguity in the Problem Statement:\n",
        "\n",
        "There is one main ambiguity in the problem statement:\n",
        "\n",
        "**Total volume vs. Number of servings:** The statement \"18 total cups of this drink\" could be interpreted in two ways:\n",
        "  * **18 cups of the combined volume:** This would mean Abigail used a total of 18 cups of liquid, including both iced tea and lemonade.\n",
        "  * **18 individual servings:** This would mean Abigail made 18 individual drinks, each containing 1/4 cup of iced tea and 1 1/4 cup of lemonade.\n",
        "\n",
        "Let us assume the interpretation \"18 cups of the combined volume\".\n",
        "\n",
        "## Solution assuming 18 cups of combined volume:\n",
        "\n",
        "**Step 1: Find the proportion of lemonade in one drink:**\n",
        "\n",
        "* Lemonade: 1 1/4 cups\n",
        "* Iced tea: 1/4 cup\n",
        "* Total: 1 1/4 + 1/4 = 1 1/2 cups\n",
        "* Lemonade proportion: (1 1/4) / (1 1/2) = 5/6\n",
        "\n",
        "**Step 2: Calculate the amount of lemonade in the pitcher:**\n",
        "\n",
        "* Total volume: 18 cups\n",
        "* Lemonade proportion: 5/6\n",
        "* Volume of lemonade: 18 * (5/6) = 15 cups\n",
        "\n",
        "Therefore, there are 15 cups of lemonade in the pitcher. The answer is 15.\n",
        "\n",
        "Q: A deep-sea monster rises from the waters once every hundred years to feast on a ship and sate its hunger. Over three hundred years, it has consumed 847 people. Ships have been built larger over time, so each new ship has twice as many people as the last ship. How many people were on the ship the monster ate in the first hundred years?\n",
        "A: Let us solve it using algebra. Let x be the number of people on the ship the monster ate in the first hundred years.\n",
        "\n",
        "The number of people on the ship eaten in the second hundred years is 2x, and in the third hundred years is 4x.\n",
        "\n",
        "Therefore, the total number of people eaten over three hundred years is x + 2x + 4x = 847.\n",
        "\n",
        "Combining like terms, we get 7x = 847.\n",
        "\n",
        "Dividing both sides by 7, we find x = 121.\n",
        "\n",
        "Therefore, there were 121 people on the ship the monster ate in the first hundred years. The answer is 121.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHfSuPoUJktI"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import re\n",
        "\n",
        "TEMPLATE = \"\"\"\n",
        "Q: {question}\n",
        "A:\"\"\"\n",
        "\n",
        "# Helper function to format GSM8K prompts\n",
        "# def format_gsm8k_prompt(question):\n",
        "#     \"\"\"\n",
        "#     Format the question as a prompt for the Gemma model.\n",
        "#     \"\"\"\n",
        "#     return f\"<start_of_turn>user\\n{question}<end_of_turn><eos>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "# Generate model answers\n",
        "def generate_answer(model, prompt, max_length=128, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Generate an answer using the Gemma model.\n",
        "    \"\"\"\n",
        "    outputs = model.generate(prompt, output_len=max_length, device=device)\n",
        "    answer = outputs.split(\"<end_of_turn>\")[0].split(\"<start_of_turn>model\\n\")[-1]\n",
        "    return answer.strip()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_gsm8k_subset(model, dataset, device=\"cuda\", num_samples=500, random_seed=42, print_info=False):\n",
        "    \"\"\"\n",
        "    Evaluate the Gemma model on a subset of GSM8K and compute accuracy.\n",
        "    \"\"\"\n",
        "    random.seed(random_seed)\n",
        "    references = []\n",
        "\n",
        "    # Randomly select `num_samples` examples\n",
        "    test_dataset = dataset[\"test\"]\n",
        "    sampled_indices = random.sample(range(len(test_dataset)), min(num_samples, len(test_dataset)))\n",
        "    sampled_data = [test_dataset[idx] for idx in sampled_indices]\n",
        "\n",
        "    responses = {}\n",
        "    idx = 0\n",
        "    correct = 0\n",
        "\n",
        "    for task_id, problem in tqdm(enumerate(sampled_data), desc=\"Evaluating GSM8K\"):\n",
        "      if task_id in responses: continue\n",
        "\n",
        "      # Formulate and print the full prompt\n",
        "      full_prompt = (PREAMBLE +'\\n\\n' + PROMPT + '\\n' +\n",
        "                    TEMPLATE.format(question=problem['question']))\n",
        "\n",
        "      response = generate_answer(model, full_prompt, max_length=128, device=device)\n",
        "      responses[task_id] = response.split('Q:')[0]\n",
        "\n",
        "      if task_id % 10 == 0 and print_info:\n",
        "        print(f\"\\nPrompt: {problem['question']}\")\n",
        "        print(f\"Answer: {find_number(problem['answer'])}\")\n",
        "        print(f\"Response: {find_number(responses[task_id])}\")\n",
        "\n",
        "      correct_answer = False\n",
        "      try:\n",
        "        correct_answer = float(maybe_remove_comma(\n",
        "            find_number(problem['answer']))) == float(responses[task_id])\n",
        "      except:\n",
        "        correct_answer = maybe_remove_comma(\n",
        "            find_number(problem['answer'])) == maybe_remove_comma(\n",
        "                find_number(responses[task_id]))\n",
        "\n",
        "      if correct_answer:\n",
        "        correct += 1\n",
        "\n",
        "      result = {\n",
        "          \"question\": problem['question'],\n",
        "          \"answer\": problem['answer'],\n",
        "          \"model_prediction\": response,\n",
        "          \"correct\": correct_answer\n",
        "      }\n",
        "      # print(json.dumps(result, indent=4))\n",
        "\n",
        "      references.append({\n",
        "          \"id\": task_id,\n",
        "          \"result\": result\n",
        "      })\n",
        "\n",
        "\n",
        "      if idx % 10 == 0 and print_info:\n",
        "        print('-'*40)\n",
        "        print(f\"Correct: {correct} out of {idx+1}\")\n",
        "        print(\"=\"*40)\n",
        "        # print(f\"Accuracy: {correct/num_samples}\")\n",
        "\n",
        "      idx += 1\n",
        "\n",
        "    path = f'percentile_{PAWS_PERCENTILE}_gsm8k_evaluation_results.json'\n",
        "    try:\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(references, f, indent=4)\n",
        "        print(f\"Evaluation results saved to {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results: {e}\")\n",
        "\n",
        "    print(f\"Accuracy: {correct/num_samples}\")\n",
        "    # return responses\n",
        "\n",
        "# Utilities\n",
        "def find_numbers(x: str) -> list[str]:\n",
        "  \"\"\"Finds all numbers in a string.\"\"\"\n",
        "  # Search for number, possibly negative (hyphen), with thousand separators\n",
        "  # (comma), and with a decimal point (period inbetween digits).\n",
        "  numbers = re.compile(\n",
        "      r'-?[\\d,]*\\.?\\d+',\n",
        "      re.MULTILINE | re.DOTALL | re.IGNORECASE,\n",
        "  ).findall(x)\n",
        "  return numbers\n",
        "\n",
        "\n",
        "def find_number(x: str,\n",
        "                answer_delimiter: str = 'The answer is') -> str:\n",
        "  \"\"\"Finds the most relevant number in a string.\"\"\"\n",
        "  # If model uses the answer delimiter, then select the first number following\n",
        "  # that format.\n",
        "  if answer_delimiter in x:\n",
        "    answer = x.split(answer_delimiter)[-1]\n",
        "    numbers = find_numbers(answer)\n",
        "    if numbers:\n",
        "      return numbers[0]\n",
        "\n",
        "  # In general, select the last number in the string.\n",
        "  numbers = find_numbers(x)\n",
        "  if numbers:\n",
        "    return numbers[-1]\n",
        "  return ''\n",
        "\n",
        "\n",
        "def maybe_remove_comma(x: str) -> str:\n",
        "  # Example: 5,600 -> 5600\n",
        "  return x.replace(',', '')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckD2FG-IKQj1",
        "outputId": "feaa44a1-c6c9-41ef-e9f3-e7795461b1fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORJ-800_6-7r",
        "outputId": "19229c33-761c-4684-b01f-dd9cefbbb249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating GSM8K: 500it [56:42,  6.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results saved to percentile_25_gsm8k_evaluation_results.json\n",
            "Accuracy: 0.014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation\n",
        "evaluate_gsm8k_subset(model, dataset, num_samples=500, print_info=PRINT_INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSg5qBLK00we",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "cef3d9db-45df-4b2f-ca1e-db51a0c3d116"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_50db4418-72ab-448e-ae44-1c2b910fd7f6\", \"percentile_25_gsm8k_evaluation_results.json\", 501969)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# retrieve json from disk\n",
        "from google.colab import files\n",
        "files.download(f'percentile_{PAWS_PERCENTILE}_gsm8k_evaluation_results.json')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}